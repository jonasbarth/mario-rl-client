{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fun.FeudalNet'>\n",
      "Loading model\n",
      "Loading model\n",
      "Loading model\n",
      "Loading model\n",
      "no shared optimizer\n",
      "Episode 0\n",
      "Episode 1\n",
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n",
      "Episode 6\n",
      "Episode 7\n",
      "Episode 8\n",
      "Episode 9\n",
      "Episode 10\n",
      "Episode 11\n",
      "Episode 12\n",
      "Episode 13\n",
      "Episode 14\n",
      "Episode 15\n",
      "Episode 16\n",
      "Episode 17\n",
      "Episode 18\n",
      "Episode 19\n",
      "Episode 20\n",
      "Episode 21\n",
      "Episode 22\n",
      "Episode 23\n",
      "Episode 24\n",
      "Episode 25\n",
      "Episode 26\n",
      "Episode 27\n",
      "Episode 28\n",
      "Episode 29\n",
      "Episode 30\n",
      "Episode 31\n",
      "Episode 32\n",
      "Episode 33\n",
      "Episode 34\n",
      "Episode 35\n",
      "Episode 36\n",
      "Episode 37\n",
      "Episode 38\n",
      "Episode 39\n",
      "Episode 40\n",
      "Episode 41\n",
      "Episode 42\n",
      "Episode 43\n",
      "Episode 44\n",
      "Episode 45\n",
      "Episode 46\n",
      "Episode 47\n",
      "Episode 48\n",
      "Episode 49\n",
      "Episode 50\n",
      "Episode 51\n",
      "Episode 52\n",
      "Episode 53\n",
      "Episode 54\n",
      "Episode 55\n",
      "Episode 56\n",
      "Episode 57\n",
      "Episode 58\n",
      "Episode 59\n",
      "Episode 60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4fd67e62304a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Uni\\Year 4\\ECM3401 Individual Literature and Review Project\\dev\\marioai\\mario-rl-client\\fun\\fun_agent.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[0mvalue_worker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_dcos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m                 \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\marioenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Year 4\\ECM3401 Individual Literature and Review Project\\dev\\marioai\\mario-rl-client\\fun\\fun.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, states, reset_value_grad)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;31m# sum on c different gt values, note that gt = normalize(hx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0msum_goal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates_M\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0msum_goal_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreset_grad2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_goal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\marioenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[0;32m   3044\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3045\u001b[0m         \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3046\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3047\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3048\u001b[0m         \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import my_optim\n",
    "from fun import FeudalNet\n",
    "\n",
    "%run fun_agent\n",
    "%run ../hyper_params\n",
    "%run ../preprocessor\n",
    "%run ../game\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Feudal Net with A3C setup')\n",
    "parser.add_argument('--lr', type=float, default=0.0003,  # try LogUniform(1e-4.5, 1e-3.5)\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--alpha', type=float, default=0.8,\n",
    "                    help='intrinsic reward multiplier')\n",
    "parser.add_argument('--gamma-worker', type=float, default=0.95,\n",
    "                    help='worker discount factor for rewards')\n",
    "parser.add_argument('--gamma-manager', type=float, default=0.99,\n",
    "                    help='manager discount factor for rewards')\n",
    "parser.add_argument('--tau-worker', type=float, default=1.00,\n",
    "                    help='parameter for GAE (worker only)')\n",
    "parser.add_argument('--entropy-coef', type=float, default=0.01,\n",
    "                    help='entropy term coefficient (also called beta)')\n",
    "parser.add_argument('--value-worker-loss-coef', type=float, default=1,\n",
    "                    help='worker value loss coefficient')\n",
    "parser.add_argument('--value-manager-loss-coef', type=float, default=1,\n",
    "                    help='manager value loss coefficient')\n",
    "parser.add_argument('--max-grad-norm', type=float, default=40,\n",
    "                    help='value loss coefficient')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--num-processes', type=int, default=4,\n",
    "                    help='how many training processes to use')\n",
    "parser.add_argument('--num-steps', type=int, default=400,\n",
    "                    help='number of forward steps in A3C')\n",
    "parser.add_argument('--max-episode-length', type=int, default=1000000,\n",
    "                    help='maximum length of an episode')\n",
    "parser.add_argument('--env-name', default='PongDeterministic-v4',\n",
    "                    help='environment to train on (default: PongDeterministic-v4)')\n",
    "parser.add_argument('--no-shared', default=False,\n",
    "                    help='use an optimizer without shared momentum.')\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "    mp.set_start_method('spawn')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    env = create_atari_env(args.env_name)\n",
    "    shared_model = FeudalNet(env.observation_space, env.action_space, channel_first=True)\n",
    "    shared_model.share_memory()\n",
    "\n",
    "    if args.no_shared:\n",
    "        optimizer = None\n",
    "    else:\n",
    "        optimizer = my_optim.SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "        optimizer.share_memory()\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    counter = mp.Value('i', 0)\n",
    "    lock = mp.Lock()\n",
    "\n",
    "    import socket\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    log_dir = os.path.join('runs', current_time + '_' + socket.gethostname())\n",
    "\n",
    "    p = mp.Process(target=test, args=(args.num_processes, shared_model, counter, log_dir, lock, args))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "    for rank in range(0, args.num_processes):\n",
    "        p = mp.Process(target=train, args=(rank, shared_model, counter, log_dir, lock, optimizer, args))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\"\"\"\n",
    "\n",
    "params = get_param_dict(\"../hyperparameters.json\")\n",
    "\n",
    "\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "seed = params[\"seed\"]\n",
    "num_steps = params[\"num_steps\"]\n",
    "gamma = params[\"gamma\"]\n",
    "gamma_worker = params[\"gamma_worker\"]\n",
    "gamma_manager = params[\"gamma_manager\"]\n",
    "alpha = params[\"alpha\"]\n",
    "tau_worker = params[\"tau_worker\"]\n",
    "entropy_coef = params[\"entropy_coef\"]\n",
    "value_worker_loss_coef = params[\"value_worker_loss_coef\"]\n",
    "value_manager_loss_coef = params[\"value_manager_loss_coef\"]\n",
    "max_grad_norm = params[\"max_grad_norm\"]\n",
    "        \n",
    "n_frames = params[\"n_frames\"]\n",
    "n_channels = params[\"n_channels\"]\n",
    "original_width = params[\"original_width\"]\n",
    "original_height = params[\"original_height\"]\n",
    "scaled_width = params[\"scaled_width\"]\n",
    "scaled_height = params[\"scaled_height\"]\n",
    "rgb = params[\"rgb\"]\n",
    "egocentric = params[\"egocentric\"]\n",
    "frame_skip = params[\"frame_skip\"]\n",
    "num_episodes = params[\"num_episodes\"]\n",
    "max_steps = params[\"max_steps\"]\n",
    "\n",
    "game_visible = params[\"game_visible\"]\n",
    "mario_scale = params[\"mario_scale\"]\n",
    "mario_state = params[\"mario_state\"]\n",
    "mario_timer = params[\"mario_timer\"]\n",
    "mario_fps = params[\"mario_fps\"]\n",
    "level_path = params[\"level_path\"]\n",
    "#level_path = \"/levels/custom/flat.txt\",\n",
    "preprocess = Preprocessor(n_frames, n_channels, original_height, original_width, scaled_height, scaled_width)\n",
    "game = Game(game_visible, mario_scale, mario_state, mario_timer, mario_fps, level_path, preprocess, rgb, egocentric, frame_skip)\n",
    "\n",
    "shared_model = FeudalNet(game.observation_space, game.action_space, channel_first=True)\n",
    "print(type(shared_model))\n",
    "\n",
    "shared_model.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_fun.pt\"))\n",
    "shared_model.worker.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_worker.pt\"))\n",
    "shared_model.manager.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_manager.pt\"))\n",
    "shared_model.perception.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_perception.pt\"))\n",
    "\n",
    "agent = FunAgent(game, shared_model, None, seed, learning_rate, num_steps, gamma, gamma_worker, gamma_manager, alpha, tau_worker, entropy_coef, value_worker_loss_coef, value_manager_loss_coef, max_grad_norm, num_episodes, max_steps)\n",
    "#agent.train()\n",
    "\n",
    "\n",
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

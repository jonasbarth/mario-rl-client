{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fun.FeudalNet'>\n",
      "Loading model\n",
      "Loading model\n",
      "Loading model\n",
      "Loading model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'max_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b6edd153d263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./models/2020_04_09_08_59_08/epochs_100_gap_4_perception.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_worker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau_worker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_worker_loss_coef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_manager_loss_coef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;31m#agent.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'max_steps'"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import my_optim\n",
    "from fun import FeudalNet\n",
    "\n",
    "%run fun_agent\n",
    "%run ../hyper_params\n",
    "%run ../preprocessor\n",
    "%run ../game\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Feudal Net with A3C setup')\n",
    "parser.add_argument('--lr', type=float, default=0.0003,  # try LogUniform(1e-4.5, 1e-3.5)\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--alpha', type=float, default=0.8,\n",
    "                    help='intrinsic reward multiplier')\n",
    "parser.add_argument('--gamma-worker', type=float, default=0.95,\n",
    "                    help='worker discount factor for rewards')\n",
    "parser.add_argument('--gamma-manager', type=float, default=0.99,\n",
    "                    help='manager discount factor for rewards')\n",
    "parser.add_argument('--tau-worker', type=float, default=1.00,\n",
    "                    help='parameter for GAE (worker only)')\n",
    "parser.add_argument('--entropy-coef', type=float, default=0.01,\n",
    "                    help='entropy term coefficient (also called beta)')\n",
    "parser.add_argument('--value-worker-loss-coef', type=float, default=1,\n",
    "                    help='worker value loss coefficient')\n",
    "parser.add_argument('--value-manager-loss-coef', type=float, default=1,\n",
    "                    help='manager value loss coefficient')\n",
    "parser.add_argument('--max-grad-norm', type=float, default=40,\n",
    "                    help='value loss coefficient')\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--num-processes', type=int, default=4,\n",
    "                    help='how many training processes to use')\n",
    "parser.add_argument('--num-steps', type=int, default=400,\n",
    "                    help='number of forward steps in A3C')\n",
    "parser.add_argument('--max-episode-length', type=int, default=1000000,\n",
    "                    help='maximum length of an episode')\n",
    "parser.add_argument('--env-name', default='PongDeterministic-v4',\n",
    "                    help='environment to train on (default: PongDeterministic-v4)')\n",
    "parser.add_argument('--no-shared', default=False,\n",
    "                    help='use an optimizer without shared momentum.')\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "    mp.set_start_method('spawn')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    env = create_atari_env(args.env_name)\n",
    "    shared_model = FeudalNet(env.observation_space, env.action_space, channel_first=True)\n",
    "    shared_model.share_memory()\n",
    "\n",
    "    if args.no_shared:\n",
    "        optimizer = None\n",
    "    else:\n",
    "        optimizer = my_optim.SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "        optimizer.share_memory()\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    counter = mp.Value('i', 0)\n",
    "    lock = mp.Lock()\n",
    "\n",
    "    import socket\n",
    "    from datetime import datetime\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    log_dir = os.path.join('runs', current_time + '_' + socket.gethostname())\n",
    "\n",
    "    p = mp.Process(target=test, args=(args.num_processes, shared_model, counter, log_dir, lock, args))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "    for rank in range(0, args.num_processes):\n",
    "        p = mp.Process(target=train, args=(rank, shared_model, counter, log_dir, lock, optimizer, args))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\"\"\"\n",
    "\n",
    "params = get_param_dict(\"../hyperparameters.json\")\n",
    "\n",
    "\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "seed = params[\"seed\"]\n",
    "num_steps = params[\"num_steps\"]\n",
    "gamma = params[\"gamma\"]\n",
    "gamma_worker = params[\"gamma_worker\"]\n",
    "gamma_manager = params[\"gamma_manager\"]\n",
    "alpha = params[\"alpha\"]\n",
    "tau_worker = params[\"tau_worker\"]\n",
    "entropy_coef = params[\"entropy_coef\"]\n",
    "value_worker_loss_coef = params[\"value_worker_loss_coef\"]\n",
    "value_manager_loss_coef = params[\"value_manager_loss_coef\"]\n",
    "max_grad_norm = params[\"max_grad_norm\"]\n",
    "        \n",
    "n_frames = params[\"n_frames\"]\n",
    "n_channels = params[\"n_channels\"]\n",
    "original_width = params[\"original_width\"]\n",
    "original_height = params[\"original_height\"]\n",
    "scaled_width = params[\"scaled_width\"]\n",
    "scaled_height = params[\"scaled_height\"]\n",
    "rgb = params[\"rgb\"]\n",
    "egocentric = params[\"egocentric\"]\n",
    "frame_skip = params[\"frame_skip\"]\n",
    "num_episodes = params[\"num_episodes\"]\n",
    "max_steps = params[\"max_steps\"]\n",
    "\n",
    "game_visible = params[\"game_visible\"]\n",
    "mario_scale = params[\"mario_scale\"]\n",
    "mario_state = params[\"mario_state\"]\n",
    "mario_timer = params[\"mario_timer\"]\n",
    "mario_fps = params[\"mario_fps\"]\n",
    "level_path = params[\"level_path\"]\n",
    "#level_path = \"/levels/custom/flat.txt\",\n",
    "preprocess = Preprocessor(n_frames, n_channels, original_height, original_width, scaled_height, scaled_width)\n",
    "game = Game(game_visible, mario_scale, mario_state, mario_timer, mario_fps, level_path, preprocess, rgb, egocentric, frame_skip)\n",
    "\n",
    "shared_model = FeudalNet(game.observation_space, game.action_space, channel_first=True)\n",
    "print(type(shared_model))\n",
    "\n",
    "shared_model.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_fun.pt\"))\n",
    "shared_model.worker.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_worker.pt\"))\n",
    "shared_model.manager.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_manager.pt\"))\n",
    "shared_model.perception.load_state_dict(game.load_model(\"./models/2020_04_09_08_59_08/epochs_100_gap_4_perception.pt\"))\n",
    "\n",
    "agent = FunAgent(game, shared_model, None, seed, learning_rate, num_steps, gamma, gamma_worker, gamma_manager, alpha, tau_worker, entropy_coef, value_worker_loss_coef, value_manager_loss_coef, max_grad_norm, num_episodes, max_steps)\n",
    "#agent.train()\n",
    "\n",
    "\n",
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

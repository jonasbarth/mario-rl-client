{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Saurabh Kumar\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import clustering\n",
    "import dqn\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import hierarchical_dqn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "tf.flags.DEFINE_string('agent_type', 'h_dqn', 'RL agent type.')\n",
    "tf.flags.DEFINE_string('logdir', 'experiment_logs/', 'Directory of logfile.')\n",
    "tf.flags.DEFINE_string('experiment_dir', '', 'Directory of experiment files.')\n",
    "tf.flags.DEFINE_string('logfile', 'log.txt', 'Name of the logfile.')\n",
    "tf.flags.DEFINE_string('env_name', 'MountainCar-v0', 'Name of the environment.')\n",
    "\n",
    "env_name = ''\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "def log(logfile, iteration, rewards):\n",
    "    \"\"\"Function that logs the reward statistics obtained by the agent.\n",
    "\n",
    "    Args:\n",
    "        logfile: File to log reward statistics.\n",
    "        iteration: The current iteration.\n",
    "        rewards: Array of rewards obtained in the current iteration.\n",
    "    \"\"\"\n",
    "    log_string = '{} {} {} {}'.format(\n",
    "        iteration, np.min(rewards), np.mean(rewards), np.max(rewards))\n",
    "    print(log_string)\n",
    "\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write(log_string + '\\n')\n",
    "\n",
    "\n",
    "def make_environment(env_name):\n",
    "    return gym.make(env_name)\n",
    "\n",
    "\n",
    "def make_agent(agent_type, env, num_clusters, use_extra_travel_penalty, use_extra_bit,\n",
    "    use_controller_dqn, use_intrinsic_timeout, use_memory, memory_size, pretrain_controller):\n",
    "    if agent_type == 'dqn':\n",
    "        return dqn.DqnAgent(state_dims=[2],\n",
    "                            num_actions=2) # env.action_space.n\n",
    "    elif agent_type == 'h_dqn':\n",
    "        meta_controller_state_fn, check_subgoal_fn, num_subgoals, subgoals = clustering.get_cluster_fn(\n",
    "            n_clusters=num_clusters, extra_bit=use_extra_bit)\n",
    "\n",
    "        return hierarchical_dqn.HierarchicalDqnAgent(\n",
    "            state_sizes=[num_subgoals, [2]],\n",
    "            agent_types=['tabular', 'network'],\n",
    "            subgoals=subgoals,\n",
    "            num_subgoals=num_subgoals,\n",
    "            num_primitive_actions=2, # env.action_space.n\n",
    "            meta_controller_state_fn=meta_controller_state_fn,\n",
    "            check_subgoal_fn=check_subgoal_fn,\n",
    "            use_extra_travel_penalty=use_extra_travel_penalty,\n",
    "            use_extra_bit_for_subgoal_center=use_extra_bit,\n",
    "            use_controller_dqn=use_controller_dqn,\n",
    "            use_intrinsic_timeout=use_intrinsic_timeout,\n",
    "            use_memory=use_memory,\n",
    "            memory_size=memory_size,\n",
    "            pretrain_controller=pretrain_controller)\n",
    "\n",
    "\n",
    "def run(env_name='MountainCar-v0',\n",
    "        agent_type='dqn',\n",
    "        num_iterations=1000,\n",
    "        num_train_episodes=100,\n",
    "        num_eval_episodes=100,\n",
    "        logdir=None,\n",
    "        experiment_dir=None,\n",
    "        logfile=None):\n",
    "    \"\"\"Function that executes RL training and evaluation.\n",
    "\n",
    "    Args:\n",
    "        env_name: Name of the environment that the agent will interact with.\n",
    "        agent_type: The type RL agent that will be used for training.\n",
    "        num_iterations: Number of iterations to train for.\n",
    "        num_train_episodes: Number of training episodes per iteration.\n",
    "        num_eval_episodes: Number of evaluation episodes per iteration.\n",
    "        logdir: Directory for log file.\n",
    "        logfile: File to log the agent's performance over training.\n",
    "    \"\"\"\n",
    "    experiment_dir += '_agent_type_' + agent_type\n",
    "\n",
    "    experiment_dir = logdir + experiment_dir\n",
    "    logfile = experiment_dir + '/' + logfile\n",
    "\n",
    "    try:\n",
    "        os.stat(experiment_dir)\n",
    "    except:\n",
    "        os.mkdir(experiment_dir)\n",
    "\n",
    "\n",
    "    env = make_environment(env_name)\n",
    "    env_test = make_environment(env_name)\n",
    "    # env_test = Monitor(env_test, directory='videos/', video_callable=lambda x: True, resume=True)\n",
    "    print 'Made environment!'\n",
    "    agent = make_agent(agent_type, env)\n",
    "    print 'Made agent!'\n",
    "\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "        # Run train episodes.\n",
    "        for train_episode in range(num_train_episodes):\n",
    "            # Reset the environment.\n",
    "            state = env.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            # Run the episode.\n",
    "            terminal = False\n",
    "\n",
    "            while not terminal:\n",
    "                action = agent.sample(state)\n",
    "                # Remove the do-nothing action.\n",
    "                if env_name == 'MountainCar-v0':\n",
    "                    if action == 1:\n",
    "                        env_action = 2\n",
    "                    else:\n",
    "                        env_action = action\n",
    "\n",
    "                next_state, reward, terminal, _ = env.step(env_action)\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "                agent.store(state, action, reward, next_state, terminal)\n",
    "                agent.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                # Update the state.\n",
    "                state = next_state\n",
    "\n",
    "        eval_rewards = []\n",
    "\n",
    "        # Run eval episodes.\n",
    "        for eval_episode in range(num_eval_episodes):\n",
    "\n",
    "            # Reset the environment.\n",
    "            state = env_test.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            # Run the episode.\n",
    "            terminal = False\n",
    "\n",
    "            while not terminal:\n",
    "    \t\tif agent_type == 'dqn':\n",
    "    \t\t    action = agent.best_action(state)\n",
    "                else:\n",
    "                    action, info = agent.best_action(state)\n",
    "                if agent_type == 'h_dqn' and info is not None:\n",
    "                    curr_state = info[0]\n",
    "                    if not use_memory:\n",
    "                        curr_state = np.where(np.squeeze(curr_state) == 1)[0][0]\n",
    "                    else:\n",
    "                        curr_state = np.squeeze(curr_state)[-1] - 1\n",
    "                    goal = info[1]\n",
    "                    heat_map[curr_state][goal] += 1\n",
    "\n",
    "                # Remove the do-nothing action.\n",
    "                if action == 1:\n",
    "                    env_action = 2\n",
    "                else:\n",
    "                    env_action = action\n",
    "\n",
    "                next_state, reward, terminal, _ = env_test.step(env_action)\n",
    "\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "                # env_test.render()\n",
    "                agent.store(state, action, reward, next_state, terminal, eval=True)\n",
    "                if reward > 1:\n",
    "                    reward = 1 # For sake of comparison.\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            eval_rewards.append(episode_reward)\n",
    "\n",
    "        with open(experiment_dir + '/eval_rewards_' + str(it), 'wb') as f:\n",
    "            pickle.dump(eval_rewards, f)\n",
    "\n",
    "        log(logfile, it, eval_rewards)\n",
    "\n",
    "\n",
    "run(agent_type=FLAGS.agent_type, logdir=FLAGS.logdir, experiment_dir=FLAGS.experiment_dir,\n",
    "    logfile=FLAGS.logfile)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

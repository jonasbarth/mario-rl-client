{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clustering'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-41229e540d13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Agg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mclustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhierarchical_dqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'clustering'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Saurabh Kumar\n",
    "\"\"\"\n",
    "\n",
    "%run ../hyper_params\n",
    "%run ../preprocessor\n",
    "%run ../game\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import clustering\n",
    "import dqn\n",
    "import hierarchical_dqn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "tf.flags.DEFINE_string('agent_type', 'h_dqn', 'RL agent type.')\n",
    "tf.flags.DEFINE_string('logdir', 'experiment_logs/', 'Directory of logfile.')\n",
    "tf.flags.DEFINE_string('experiment_dir', '', 'Directory of experiment files.')\n",
    "tf.flags.DEFINE_string('logfile', 'log.txt', 'Name of the logfile.')\n",
    "tf.flags.DEFINE_string('env_name', 'MountainCar-v0', 'Name of the environment.')\n",
    "\n",
    "env_name = ''\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "def log(logfile, iteration, rewards):\n",
    "    \"\"\"Function that logs the reward statistics obtained by the agent.\n",
    "\n",
    "    Args:\n",
    "        logfile: File to log reward statistics.\n",
    "        iteration: The current iteration.\n",
    "        rewards: Array of rewards obtained in the current iteration.\n",
    "    \"\"\"\n",
    "    log_string = '{} {} {} {}'.format(\n",
    "        iteration, np.min(rewards), np.mean(rewards), np.max(rewards))\n",
    "    print(log_string)\n",
    "\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write(log_string + '\\n')\n",
    "\n",
    "\n",
    "def make_environment(env_name):\n",
    "    params = get_param_dict(\"../hyperparameters.json\")\n",
    "    n_frames = params[\"n_frames\"]\n",
    "    n_channels = params[\"n_channels\"]\n",
    "    original_width = params[\"original_width\"]\n",
    "    original_height = params[\"original_height\"]\n",
    "    scaled_width = params[\"scaled_width\"]\n",
    "    scaled_height = params[\"scaled_height\"]\n",
    "    rgb = params[\"rgb\"]\n",
    "\n",
    "    game_visible = params[\"game_visible\"]\n",
    "    mario_scale = params[\"mario_scale\"]\n",
    "    mario_state = params[\"mario_state\"]\n",
    "    mario_timer = params[\"mario_timer\"]\n",
    "    mario_fps = params[\"mario_fps\"]\n",
    "    level_path = params[\"level_path\"]\n",
    "    preprocess = Preprocessor(n_frames, n_channels, original_height, original_width, scaled_height, scaled_width)\n",
    "    game = Game(game_visible, mario_scale, mario_state, mario_timer, mario_fps, level_path, preprocess, rgb, True, 4)\n",
    "    return game\n",
    "\n",
    "\n",
    "def make_agent(agent_type, env, num_clusters, use_extra_travel_penalty, use_extra_bit,\n",
    "    use_controller_dqn, use_intrinsic_timeout, use_memory, memory_size, pretrain_controller):\n",
    "    if agent_type == 'dqn':\n",
    "        return dqn.DqnAgent(state_dims=env.observation_space,\n",
    "                            num_actions=len(env.action_space)) # env.action_space.n\n",
    "    elif agent_type == 'h_dqn':\n",
    "        meta_controller_state_fn, check_subgoal_fn, num_subgoals, subgoals = clustering.get_cluster_fn(\n",
    "            n_clusters=num_clusters, extra_bit=use_extra_bit)\n",
    "\n",
    "        return hierarchical_dqn.HierarchicalDqnAgent(\n",
    "            state_sizes=[num_subgoals, [2]],\n",
    "            agent_types=['tabular', 'network'],\n",
    "            subgoals=subgoals,\n",
    "            num_subgoals=num_subgoals,\n",
    "            num_primitive_actions=2, # env.action_space.n\n",
    "            meta_controller_state_fn=meta_controller_state_fn,\n",
    "            check_subgoal_fn=check_subgoal_fn,\n",
    "            use_extra_travel_penalty=use_extra_travel_penalty,\n",
    "            use_extra_bit_for_subgoal_center=use_extra_bit,\n",
    "            use_controller_dqn=use_controller_dqn,\n",
    "            use_intrinsic_timeout=use_intrinsic_timeout,\n",
    "            use_memory=use_memory,\n",
    "            memory_size=memory_size,\n",
    "            pretrain_controller=pretrain_controller)\n",
    "\n",
    "\n",
    "def run(env_name='MountainCar-v0',\n",
    "        agent_type='h_dqn',\n",
    "        num_iterations=1,\n",
    "        num_train_episodes=1,\n",
    "        num_eval_episodes=1,\n",
    "        logdir=None,\n",
    "        experiment_dir=None,\n",
    "        logfile=None):\n",
    "    \"\"\"Function that executes RL training and evaluation.\n",
    "\n",
    "    Args:\n",
    "        env_name: Name of the environment that the agent will interact with.\n",
    "        agent_type: The type RL agent that will be used for training.\n",
    "        num_iterations: Number of iterations to train for.\n",
    "        num_train_episodes: Number of training episodes per iteration.\n",
    "        num_eval_episodes: Number of evaluation episodes per iteration.\n",
    "        logdir: Directory for log file.\n",
    "        logfile: File to log the agent's performance over training.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    experiment_dir += '_agent_type_' + agent_type\n",
    "\n",
    "    experiment_dir = logdir + experiment_dir\n",
    "    logfile = experiment_dir + '/' + logfile\n",
    "\n",
    "    try:\n",
    "        os.stat(experiment_dir)\n",
    "    except:\n",
    "        os.mkdir(experiment_dir)\n",
    "    \"\"\"\n",
    "\n",
    "    env = make_environment(env_name)\n",
    "    #env_test = make_environment(env_name)\n",
    "    # env_test = Monitor(env_test, directory='videos/', video_callable=lambda x: True, resume=True)\n",
    "    print('Made environment!')\n",
    "    agent = make_agent(agent_type, env)\n",
    "    print('Made agent!')\n",
    "\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "        # Run train episodes.\n",
    "        for train_episode in range(num_train_episodes):\n",
    "            # Reset the environment.\n",
    "            state, reward, status = env.start_state()\n",
    "            #state = np.expand_dims(state, axis=0)\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            # Run the episode.\n",
    "            terminal = False\n",
    "\n",
    "            while not terminal:\n",
    "                action = agent.sample(state)\n",
    "                # Remove the do-nothing action.\n",
    "                if env_name == 'MountainCar-v0':\n",
    "                    if action == 1:\n",
    "                        env_action = 2\n",
    "                    else:\n",
    "                        env_action = action\n",
    "\n",
    "                next_state, reward, status = env.step(env_action)\n",
    "                terminal = not env.status_to_bool(status)\n",
    "                #next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "                agent.store(state, action, reward, next_state, terminal)\n",
    "                agent.update()\n",
    "\n",
    "                episode_reward += reward\n",
    "                # Update the state.\n",
    "                state = next_state\n",
    "\n",
    "        eval_rewards = []\n",
    "\n",
    "        # Run eval episodes.\n",
    "        for eval_episode in range(num_eval_episodes):\n",
    "\n",
    "            # Reset the environment.\n",
    "            state = env_test.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "            episode_reward = 0\n",
    "\n",
    "            # Run the episode.\n",
    "            terminal = False\n",
    "\n",
    "            while not terminal:\n",
    "                if agent_type == 'dqn':\n",
    "                    action = agent.best_action(state)\n",
    "                else:\n",
    "                    action, info = agent.best_action(state)\n",
    "                if agent_type == 'h_dqn' and info is not None:\n",
    "                    curr_state = info[0]\n",
    "                    if not use_memory:\n",
    "                        curr_state = np.where(np.squeeze(curr_state) == 1)[0][0]\n",
    "                    else:\n",
    "                        curr_state = np.squeeze(curr_state)[-1] - 1\n",
    "                    goal = info[1]\n",
    "                    heat_map[curr_state][goal] += 1\n",
    "\n",
    "                # Remove the do-nothing action.\n",
    "                if action == 1:\n",
    "                    env_action = 2\n",
    "                else:\n",
    "                    env_action = action\n",
    "\n",
    "                next_state, reward, terminal, _ = env_test.step(env_action)\n",
    "\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "                # env_test.render()\n",
    "                agent.store(state, action, reward, next_state, terminal, eval=True)\n",
    "                if reward > 1:\n",
    "                    reward = 1 # For sake of comparison.\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            eval_rewards.append(episode_reward)\n",
    "\n",
    "        with open(experiment_dir + '/eval_rewards_' + str(it), 'wb') as f:\n",
    "            pickle.dump(eval_rewards, f)\n",
    "\n",
    "        log(logfile, it, eval_rewards)\n",
    "\n",
    "\n",
    "run(agent_type=FLAGS.agent_type, logdir=FLAGS.logdir, experiment_dir=FLAGS.experiment_dir,\n",
    "    logfile=FLAGS.logfile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
